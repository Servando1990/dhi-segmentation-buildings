{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Res-Unet with Distance Module network: RUBV3D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s161362/.conda/envs/env_dhi/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IOU_computations import *\n",
    "from Data_Handle.dataset_generator import Dataset_sat\n",
    "from predict_and_evaluate import *\n",
    "from Data_Handle.data_augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to update in real time the notebook figures\n",
    "%matplotlib notebook \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DON'T CHANGE THIS ######################\n",
    "\n",
    "\n",
    "INPUT_CHANNELS=9 #9 channels for panchromatic + 8 pansharpened. If not set to 9, plotting of patches will mess up.\n",
    "                # so only works for INPUT_CHANNELS=9 anyway.\n",
    "    ### if you change the number of channels and want to train on over types of inputs, please check the code \n",
    "#### of predict_and_evaluate.Plot_patches.produce_pansharp() to be able to prepare a pansharp with with 3 channels\n",
    "\n",
    "NB_CLASSES=2 #Building and Background. Only works for NB_CLASSES=2 anyway, otherwise this network doesn't work.\n",
    "                ### this is due to the set up in dataset_generator.parse_images()\n",
    "    \n",
    "SIZE_PATCH=128# patches of size 128x128. Needs to be equal to the size of the patches of the dataset.\n",
    "############## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PATH TO STORE MODEL ############\n",
    "GLOBAL_PATH='MODEL_DUMMY_TEST_SPACENET/'\n",
    "\n",
    "\n",
    "if not os.path.exists(GLOBAL_PATH):\n",
    "            os.makedirs(GLOBAL_PATH)\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MODEL_PATH_SAVE=GLOBAL_PATH+'RESUNET_test_'\n",
    "MODEL_PATH_RESTORE='' #Path of Model to restore ex: 'TRAINED_MODELS/RUBV3D2_final_model_ghana.pth'\n",
    "TEST_SAVE=GLOBAL_PATH+'TEST_SAVE/' #to store some patches initial and final epoch of validation set + models + performance curves\n",
    "if not os.path.exists(TEST_SAVE):\n",
    "            os.makedirs(TEST_SAVE)\n",
    "        \n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "######### PARAMETERS TO TUNE  #####\n",
    "\n",
    "DROPOUT=0.35\n",
    "DEFAULT_BATCH_SIZE =32 #has to be set to 8 for Ghana dataset and can be set up to 32 for Spacenet dataset\n",
    "DEFAULT_EPOCHS =6\n",
    "DEFAULT_VALID=32  # Batch size for validation set. \n",
    "                #Knowing that around 1200 elements in ghana validation and 15000 in spacenet validation\n",
    "\n",
    "DISPLAY_STEP=100 #how often (in terms of iterations) is displayed measures during an epoch\n",
    "IOU_STEP=2 # how often is computed IOU measures over validations et\n",
    "MAX_VAL_SIZE=200 #None if full validation dataset wants to be considered. \n",
    "                # the notebook should be used for debug and that's why small dataset size is used\n",
    "MAX_TRAIN_SIZE=1000 #None if full training dataset wants to be considered to train\n",
    "                # the notebook should be used for debug and that's why small dataset size is used\n",
    "\n",
    "###############\n",
    "DEFAULT_LAYERS=3 #number of layers of the UNET (not considering bottom layer) = number of downsmapling stages\n",
    "DEFAULT_FEATURES_ROOT=32 # number of filters in the first layer of the Unet\n",
    "DEFAULT_BN=True # Batch normalization layers included\n",
    "\n",
    "#####\n",
    "\n",
    "DEFAULT_FILTER_WIDTH=3 #convolution kernel size. ex, here: 3x3\n",
    "DEFAULT_LR=1e-3#1e-3for spacenet and ghana\n",
    "DEFAULT_N_RESBLOCKS=1 #can add residual blocks inside each stage. Make the network heavier. Not advised.\n",
    "\n",
    "###Tune Learning rate\n",
    "REDUCE_LR_STEPS = [1,5, 50, 100,200] #reduce everytime one of these epochs is reached\n",
    "\n",
    "################\n",
    "\n",
    "\n",
    "DISTANCE_NET='v2' #can be set to none if no distance module wants to be used\n",
    "BINS=10\n",
    "THRESHOLD=20\n",
    "\n",
    "if DISTANCE_NET is None:\n",
    "    DISTANCE_NET_UNET=False # has to be set to False if no distance module is used, otherwise error.\n",
    "else:\n",
    "    DISTANCE_NET_UNET=True\n",
    "\n",
    "\n",
    "##### Data augmentation set for training ###\n",
    "DATA_AUG=None\n",
    "# DATA_AUG=transforms.Compose([Transform(),ToTensor()])\n",
    "\n",
    "####### TMP folder for IOU ###\n",
    "## not to worry about, compulsory for vectorizing masks \"\"\n",
    "\n",
    "TMP_IOU=TEST_SAVE+'TMP_IOU/'\n",
    "if not os.path.exists(TMP_IOU):\n",
    "            os.makedirs(TMP_IOU)\n",
    "\n",
    "#######  Data: where the dataset is stored ###\n",
    "root_folder ='../SPACENET_DATA/SPACENET_DATA_PROCESSED/DATASET/128_x_128_8_bands_pansh/'\n",
    "# root_folder = '../2_DATA_GHANA/DATASET/128_x_128_8_pansh/'\n",
    "\n",
    "#type of loss used \n",
    "LOSS_FN='cross-entropy'# or 'jaccard_approx'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RUBV3D2 import UNet \n",
    "model=UNet(INPUT_CHANNELS,NB_CLASSES,depth =DEFAULT_LAYERS,n_features_zero =DEFAULT_FEATURES_ROOT,width_kernel=DEFAULT_FILTER_WIDTH,dropout=DROPOUT,distance_net=DISTANCE_NET_UNET,bins=BINS,batch_norm=DEFAULT_BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Draw learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used to print live the training loss, avg training loss after each epoch and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Draw_learning(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fig,self.axs=plt.subplots(1, 3,figsize=(9,3))\n",
    "        self.initialize()\n",
    "    def initialize(self):\n",
    "\n",
    "        self.axs[0].set_ylim(0,0.1)\n",
    "        self.axs[0].set_title('Loss train')\n",
    "        self.axs[1].set_ylim(0,0.1)\n",
    "        self.axs[1].set_title('Avg loss train')\n",
    "        self.axs[2].set_ylim(0,1)\n",
    "        self.axs[2].set_title('Learning rate')\n",
    "        \n",
    "        \n",
    "    def draw_update(self,loss,avg_loss_train,lr):\n",
    "\n",
    "        global_step=len(loss)\n",
    "        epoch=np.arange(len(avg_loss_train))\n",
    "        ite=np.arange(global_step)\n",
    "        self.axs[0].clear()\n",
    "        self.axs[1].clear()\n",
    "        self.axs[2].clear()\n",
    "        line1, = self.axs[0].plot(ite, loss, 'r-') \n",
    "        line1, = self.axs[1].plot(epoch, avg_loss_train, 'g-') \n",
    "        line2, = self.axs[2].plot(ite, lr, 'b-') \n",
    "        self.fig.canvas.draw()\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trains a unet instance\n",
    "    \n",
    "    :param net: the unet instance to train\n",
    "    :param batch_size: size of training batch\n",
    "    :param lr: learning rate\n",
    "    :nb_classes: always set to 2 ->background and building\n",
    "    :type of loss: 'cross-entropy' or 'jaccard_approx-approx'\n",
    "    \"\"\"\n",
    "    def __init__(self, net, batch_size=32, lr=0.001,nb_classes=2,loss_fn=LOSS_FN):\n",
    "        self.net = net\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.nb_classes=nb_classes\n",
    "        self.loss_fn=loss_fn\n",
    "    def _initialize(self, prediction_path,store_learning,iou_step,dist_net,threshold,bins):\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.net.parameters(),lr=self.lr)\n",
    "        self.prediction_path = prediction_path\n",
    "        self.store_learning=store_learning\n",
    "        self.IOU_STEP=iou_step\n",
    "        self.threshold=threshold\n",
    "        self.bins=bins\n",
    "        self.dist_net=dist_net\n",
    "        \n",
    "    def train(self, data_provider_path,store_learning, save_path='', restore_path='',  epochs=3, dropout=0.2, display_step=100, validation_batch_size=30, prediction_path = '',dist_net=None,threshold=20,bins=15,iou_step=1,reduce_lr_steps=[1,10,100,200],data_aug=None):\n",
    "        \"\"\"\n",
    "        Lauches the training process\n",
    "        \n",
    "        :param data_provider_path: where the DATASET folder is\n",
    "        :param store_learning: to store the metrics during the training as .txt file\n",
    "        :param save_path: path where to store checkpoints\n",
    "        :param restore_path: path where is the model to restore is stored\n",
    "        :param epochs: number of epochs\n",
    "        :param dropout: dropout probability\n",
    "        :param validation_batch_size: batch size of the validation set\n",
    "        :param prediction_path: where to store output of training (patches, losses .txt file, models)\n",
    "        :param dist_net: distance module or not \n",
    "        :param threshold: threshold of distance module\n",
    "        :param bins: number of bins for distance module\n",
    "        :iou_step: how often is computed Iou measures over the validation set\n",
    "        :reduce_lr_steps: epoch at which the learning rate is halved\n",
    "        :data_aug: 'yes' or 'no' if the training set is augmented\n",
    "        \"\"\"\n",
    "        \n",
    "        ##SET UP PATHS FOR TRAINING ##\n",
    "        #check they exist?\n",
    "        PATH_TRAINING=data_provider_path+'TRAINING/'\n",
    "        if not os.path.exists(PATH_TRAINING):\n",
    "            print('Training dataset path not valid. Should be path_to_dataset/TRAINING/ and this folder should contain INTPUT/ and OUTPUT/')\n",
    "            raise\n",
    "        PATH_VALIDATION=data_provider_path+'VALIDATION/'\n",
    "        if not os.path.exists(PATH_VALIDATION):\n",
    "            print('Validation dataset path not valid. Should be path_to_dataset/VALIDATION/ and this folder should contain INTPUT/ and OUTPUT/')\n",
    "            raise\n",
    "        PATH_TEST=data_provider_path+'TEST/'\n",
    "        if not os.path.exists(PATH_TEST):\n",
    "            print('Test dataset path not valid. Should be path_to_dataset/TEST/ and this folder should contain INTPUT/ and OUTPUT/')\n",
    "            raise\n",
    "        \n",
    "        \n",
    "        lr_train=[] \n",
    "        loss_train=[]\n",
    "        draw_learning=Draw_learning()\n",
    "       \n",
    "        \n",
    "        if epochs == 0:\n",
    "            print('Epoch set 0, model won\\'t be trained')\n",
    "            raise \n",
    "        if save_path=='':\n",
    "            print('Specify a path where to store the Model')\n",
    "            raise\n",
    "        \n",
    "        if prediction_path=='':\n",
    "            print('Specify where to stored visualization of training')\n",
    "            raise\n",
    "            \n",
    "        if restore_path=='':\n",
    "            store_learning.initialize('w')\n",
    "            print('Model trained from scratch')\n",
    "        else:\n",
    "            store_learning.initialize('a')\n",
    "            self.net.load_state_dict(torch.load(restore_path))\n",
    "            print('Model loaded from {}'.format(restore_path))\n",
    "            \n",
    "        self._initialize(prediction_path,store_learning,iou_step,dist_net,threshold,bins)\n",
    "        \n",
    "    \n",
    "            \n",
    "        ###Validation loader\n",
    "\n",
    "        val_generator=Dataset_sat.from_root_folder(PATH_VALIDATION,self.nb_classes,max_data_size=MAX_VAL_SIZE)\n",
    "        val_loader = DataLoader(val_generator, batch_size=validation_batch_size,shuffle=False, num_workers=1)\n",
    "        #choose a random batch to display\n",
    "        RBD=randint(0,int(val_loader.__len__())-1)\n",
    "        #compute metrics for first init forward pass in the network over the validations set\n",
    "        self.info_validation(val_loader,-1,RBD,\"_init\")\n",
    "\n",
    "        ###Training loader\n",
    "\n",
    "        train_generator=Dataset_sat.from_root_folder(PATH_TRAINING,self.nb_classes,max_data_size=MAX_TRAIN_SIZE,transform=data_aug)#max_data_size=4958 \n",
    "        \n",
    "        \n",
    "        logging.info(\"Start optimization\")\n",
    "\n",
    "        counter=0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            ##tune learning reate\n",
    "            if epoch in reduce_lr_steps:\n",
    "                self.lr = self.lr * 0.5\n",
    "                self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "            \n",
    "            total_loss = 0\n",
    "            error_tot=0   \n",
    "            train_loader = DataLoader(train_generator, batch_size=self.batch_size,shuffle=True, num_workers=1)\n",
    "            for i_batch,sample_batch in enumerate(train_loader):\n",
    "                \n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                predict_net=Train_or_Predict(sample_batch,self.dist_net,self.loss_fn,self.threshold,self.bins,self.net)\n",
    "                loss,_,probs_seg=predict_net.forward_pass()\n",
    " \n",
    "                loss,self.optimizer,self.net=predict_net.backward_prog(loss,self.optimizer)\n",
    "                \n",
    "                total_loss+=loss.data[0]\n",
    "                loss_train.append(loss.data[0])\n",
    "                lr_train.append(self.lr)\n",
    "                \n",
    "                counter+=1\n",
    "                \n",
    "                if i_batch % display_step == 0:\n",
    "                    self.output_training_stats(i_batch,loss,predict_net.batch_y,probs_seg)\n",
    "                    draw_learning.draw_update(loss_train,(self.store_learning).avg_loss_train,lr_train)\n",
    "                    \n",
    "                \n",
    "            \n",
    "            avg_loss_train_value=total_loss/train_loader.__len__()\n",
    "            (self.store_learning).avg_loss_train.append(avg_loss_train_value)\n",
    "            (self.store_learning).write_file((self.store_learning).file_train,avg_loss_train_value)\n",
    "            logging.info(\" Training {:}, Minibatch Loss= {:.4f}\".format(\"epoch_%s\"%epoch,avg_loss_train_value))\n",
    "            self.info_validation(val_loader,epoch,RBD,\"epoch_%s\"%epoch)            \n",
    "            torch.save(self.net.state_dict(),save_path + 'CP{}.pth'.format(epoch))\n",
    "            \n",
    "            print('Checkpoint {} saved !'.format(epoch))\n",
    "    \n",
    "        self.info_validation(val_loader,-2,RBD,'_last_')\n",
    "#         time.sleep(4)\n",
    "#         plt.close(fig)\n",
    "        return save_path + 'CP{}.pth'.format(epoch)\n",
    "        \n",
    "\n",
    "    def output_training_stats(self, step, loss,batch_y,probs_seg):\n",
    "    # Calculate batch loss and accuracy\n",
    "        loss_v=loss.data[0]\n",
    "        groundtruth_seg_v=np.asarray(batch_y)\n",
    "        prediction_seg_v=probs_seg.data.cpu().numpy()\n",
    "  \n",
    "    \n",
    "        logging.info(\"Iter {:}, Minibatch Loss= {:.4f}, Minibatch error= {:.4f}%\".format(step,loss_v,error_rate(prediction_seg_v, groundtruth_seg_v)))\n",
    "    \n",
    "    def info_validation(self,val_loader,epoch,RBD,name):\n",
    "\n",
    "        loss_v=0\n",
    "        error_rate_v=0\n",
    "        iou_acc_v=0\n",
    "        f1_v=0\n",
    "        if name==\"_init\":\n",
    "            display_patches=True\n",
    "            save_patches=True\n",
    "            save_IOU_metrics=False\n",
    "        elif name=='_last_':\n",
    "            display_patches=True\n",
    "            save_patches=True\n",
    "            save_IOU_metrics=False\n",
    "        else:\n",
    "            display_patches=True\n",
    "            save_patches=False\n",
    "            save_IOU_metrics=True\n",
    "            \n",
    "        \n",
    "       \n",
    "        for i_batch,sample in enumerate(val_loader):\n",
    "\n",
    "            predict_net=Train_or_Predict(sample,self.dist_net,self.loss_fn,self.threshold,self.bins,self.net)\n",
    "            loss,probs_dist,probs_seg=predict_net.forward_pass()\n",
    "            \n",
    "            prediction_seg_v=probs_seg.data.cpu().numpy()\n",
    "            groundtruth_seg_v=np.asarray(predict_net.batch_y)\n",
    "            prediction_dist_v=probs_dist.data.cpu().numpy()\n",
    "            groundtruth_dist=np.asarray(predict_net.batch_y_dist)\n",
    "            plot_patches=Plot_patches(prediction_seg_v,groundtruth_seg_v,prediction_dist_v,groundtruth_dist)\n",
    "            \n",
    "            loss_v+=loss.data[0]\n",
    "            error_rate_v+=error_rate(prediction_seg_v,groundtruth_seg_v)\n",
    "            \n",
    "        \n",
    "            if i_batch==RBD:\n",
    "                batch_x=np.asarray(predict_net.batch_x)\n",
    "                plot_patches.plot_patches_with_gt(batch_x,name,self.prediction_path,save_patches)\n",
    "                \n",
    "            if (save_IOU_metrics and (epoch+1)%self.IOU_STEP==0):\n",
    "                iou_acc,f1,_=predict_score_batch(TMP_IOU,np.argmax(groundtruth_seg_v,3),np.argmax(prediction_seg_v,3))\n",
    "                iou_acc_v+=iou_acc\n",
    "                f1_v+=f1\n",
    "        \n",
    "        loss_v/=val_loader.__len__()   \n",
    "        error_rate_v/=val_loader.__len__()  \n",
    "        logging.info(\"Verification  loss= {:.4f},error= {:.4f}%\".format(loss_v,error_rate_v))\n",
    "        \n",
    "        if (name!=\"_init\" and name!='_last_'):\n",
    "            (self.store_learning).write_file((self.store_learning).file_verif,loss_v)\n",
    "            (self.store_learning).write_file((self.store_learning).error_rate_file_verif,error_rate_v)\n",
    "\n",
    "        \n",
    "        if (save_IOU_metrics and (epoch+1)%self.IOU_STEP==0):\n",
    "            iou_acc_v/=val_loader.__len__()  \n",
    "            f1_v/=val_loader.__len__()  \n",
    "            logging.info(\"Verification   IOU Precision = {:.4f}%, F1 IOU= {:.4f}%\".format(iou_acc_v,f1_v))\n",
    "            (self.store_learning).write_file((self.store_learning).IOU_acc_file_verif,iou_acc_v)\n",
    "            (self.store_learning).write_file((self.store_learning).f1_IOU_file_verif,f1_v)\n",
    "            \n",
    "                         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "    \n",
    "cudnn.benchmark = True\n",
    "\n",
    "print('### Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "trainer=Trainer(model,DEFAULT_BATCH_SIZE,DEFAULT_LR,NB_CLASSES,LOSS_FN)\n",
    "store_learning=Store_learning(GLOBAL_PATH)\n",
    "save_path=trainer.train( root_folder,store_learning, MODEL_PATH_SAVE, MODEL_PATH_RESTORE,DEFAULT_EPOCHS,DROPOUT, DISPLAY_STEP, DEFAULT_VALID, TEST_SAVE,DISTANCE_NET,THRESHOLD,BINS,IOU_STEP,REDUCE_LR_STEPS,DATA_AUG)\n",
    "print('Last model saved is %s: '%save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_learning.file_verif.close()\n",
    "# store_learning.file_train.close()\n",
    "# store_learning.error_rate_file_verif.close()\n",
    "# store_learning.IOU_acc_file_verif.close()\n",
    "# store_learning.f1_IOU_file_verif.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_dhi]",
   "language": "python",
   "name": "conda-env-env_dhi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
